@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  date = {2010-03-31},
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/glorot10a.html},
  urldate = {2023-04-11},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  eventtitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/felix/paper/2010_Glorot_Bengio/Glorot_Bengio_2010_Understanding the difficulty of training deep feedforward neural networks.pdf}
}

@article{mackayComparisonApproximateMethods1999,
  title = {Comparison of {{Approximate Methods}} for {{Handling Hyperparameters}}},
  author = {MacKay, David J. C.},
  date = {1999-07},
  journaltitle = {Neural Computation},
  volume = {11},
  number = {5},
  pages = {1035--1068},
  issn = {0899-7667},
  doi = {10.1162/089976699300016331},
  abstract = {I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models that include unknown hyperparameters such as regularization constants and noise levels. In the evidence framework, the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized hyperparameters are used to define a gaussian approximation to the posterior distribution. In the alternative MAP method, the true posterior probability is found by integrating over the hyperparameters. The true posterior is then maximized over the model parameters, and a gaussian approximation is made. The similarities of the two approaches and their relative merits are discussed, and comparisons are made with the ideal hierarchical Bayesian solution. In moderately ill-posed problems, integration over hyperparameters yields a probability distribution with a skew peak, which causes signifi-cant biases to arise in the MAP method. In contrast, the evidence framework is shown to introduce negligible predictive error under straightforward conditions. General lessons are drawn concerning inference in many dimensions.},
  eventtitle = {Neural {{Computation}}},
  file = {/Users/felix/paper/1999_MacKay/MacKay_1999_Comparison of Approximate Methods for Handling Hyperparameters.pdf;/Users/felix/Zotero/storage/S7NCCGWB/6790802.html}
}

@book{mullerCollectingSpatialData2007,
  title = {Collecting {{Spatial Data}}: {{Optimum Design}} of {{Experiments}} for {{Random Fields}}},
  shorttitle = {Collecting {{Spatial Data}}},
  author = {Muller, Werner},
  date = {2007},
  edition = {3. Aufl., Third Revised and Extended Edition},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-31175-1},
  abstract = {The book is concerned with the statistical theory for locating spatial sensors. It bridges the gap between spatial statistics and optimum design theory. After introductions to those two fields the topics of exploratory designs and designs for spatial trend and variogram estimation are treated. Special attention is devoted to describing new methodologies to cope with the problem of correlated observations. A great number of relevant references are collected and put into a common perspective. The theoretical investigations are accompanied by a practical example, the redesign of an Upper-Austrian air pollution monitoring network. A reader should be able to find respective theory and recommendations on how to efficiently plan a specific purpose spatial monitoring network. The third edition takes into account the rapid development in the area of spatial statistics by including new relevant research and references. The revised edition contains additional material on design for detecting spatial dependence and for estimating parametrized covariance functions.},
  isbn = {978-3-540-31174-4},
  langid = {english},
  keywords = {{Earth Sciences, general},Economics,Economics and Finance,Economics\_xStatistics,Experimental design,Math. Appl. in Environmental Science,Random fields,Regional/Spatial Science,{Statistics for Business, Management, Economics, Finance, Insurance},{Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences}},
  file = {/Users/felix/paper/2007_Muller/Muller_2007_Collecting Spatial Data.pdf}
}

@article{sampsonNonparametricEstimationNonstationary1992,
  title = {Nonparametric {{Estimation}} of {{Nonstationary Spatial Covariance Structure}}},
  author = {Sampson, Paul D. and Guttorp, Peter},
  date = {1992-03-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {87},
  number = {417},
  pages = {108--119},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1992.10475181},
  urldate = {2022-11-25},
  abstract = {Estimation of the covariance structure of spatial processes is a fundamental prerequisite for problems of spatial interpolation and the design of monitoring networks. We introduce a nonparametric approach to global estimation of the spatial covariance structure of a random function Z(x, t) observed repeatedly at times ti (i = 1, …, T) at a finite number of sampling stations xi (i = 1, 2, …, N) in the plane. Our analyses assume temporal stationarity but do not assume spatial stationarity (or isotropy). We analyze the spatial dispersions var(Z(xi, t) − Z(xj, t)) as a natural metric for the spatial covariance structure and model these as a general smooth function of the geographic coordinates of station pairs (xi, xj ). The model is constructed in two steps. First, using nonmetric multidimensional scaling (MDS) we compute a two-dimensional representation of the sampling stations for which a monotone function of interpoint distances δij approximates the spatial dispersions. MDS transforms the problem into one for which the covariance structure, expressed in terms of spatial dispersions, is stationary and isotropic. Second, we compute thin-plate splines to provide smooth mappings of the geographic representation of the sampling stations into their MDS representation. The composition of this mapping f and a monotone function g derived from MDS yields a nonparametric estimator of var(Z(xa, t) − Z(xb, t)) for any two geographic locations xa and xb (monitored or not) of the form g(|f(xa ) − f(xb )|). By restricting the monotone function g to a class of conditionally nonpositive definite variogram functions, we ensure that the resulting nonparametric model corresponds to a nonnegative definite covariance model. We use biorthogonal grids, introduced by Bookstein in the field of morphometrics, to depict the thin-plate spline mappings that embody the nature of the anisotropy and nonstationarity in the sample covariance matrix. An analysis of mesoscale variability in solar radiation monitored in southwestern British Columbia demonstrates this methodology.},
  keywords = {Biorthogonal grids,Dispersion,Kriging,Multidimensional scaling,Thin-plate spline,Variogram},
  file = {/Users/felix/paper/1992_Sampson_Guttorp/Sampson_Guttorp_1992_Nonparametric Estimation of Nonstationary Spatial Covariance Structure.pdf}
}

@article{warrickOptimizationSamplingLocations1987,
  title = {Optimization of Sampling Locations for Variogram Calculations},
  author = {Warrick, A. W. and Myers, D. E.},
  date = {1987},
  journaltitle = {Water Resources Research},
  volume = {23},
  number = {3},
  pages = {496--500},
  issn = {1944-7973},
  doi = {10.1029/WR023i003p00496},
  urldate = {2022-11-25},
  abstract = {A method is presented and demonstrated for optimizing the selection of sample locations for variogram estimation. It is assumed that the distribution of distance classes is decided a priori and the problem therefore is to closely approximate the preselected distribution, although the dispersion within individual classes can also be considered. All of the locations may be selected or points added to an existing set of sites or to those chosen on regular patterns. In the examples, the sum of squares characterizing the deviation from the desired distribution of couples is reduced by as much as 2 orders of magnitude between random and optimized points. The calculations may be carried out on a microcomputer. Criteria for what constitutes best estimators for variogram are discussed, but a study of variogram estimators is not the object of this paper.},
  langid = {english},
  file = {/Users/felix/paper/1987_Warrick_Myers/Warrick_Myers_1987_Optimization of sampling locations for variogram calculations.pdf;/Users/felix/Zotero/storage/99CUC7HK/WR023i003p00496.html}
}

@book{williamsGaussianProcessesMachine2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Williams, Christopher K.I. and Rasmussen, Carl Edward},
  date = {2006},
  series = {Adaptive Computation and Machine Learning},
  edition = {2},
  number = {3},
  publisher = {{MIT press Cambridge, MA}},
  url = {http://gaussianprocess.org/gpml/chapters/RW.pdf},
  urldate = {2023-04-06},
  isbn = {0-262-18253-X},
  langid = {english},
  pagetotal = {248},
  file = {/Users/felix/paper/2006_Williams_Rasmussen/Williams_Rasmussen_2006_Gaussian processes for machine learning2.pdf}
}
